name: Scheduled Drops (Prod)

on:
  schedule:
    # 09:00 America/Sao_Paulo == 12:00 UTC (GitHub cron is UTC)
    - cron: "0 12 * * *"
  workflow_dispatch:

concurrency:
  group: scheduled-drops-prod
  cancel-in-progress: false

jobs:
  drop:
    runs-on: [self-hosted, mgc-prod]

    env:
      # Persistent prod DB
      MGC_DB: /var/lib/mgc/db.sqlite

      # Keep these for later if/when you switch to filesystem provider
      MGC_FS_PROVIDER_DIR: /var/lib/mgc/audio_pool

      # Ensure scheduled runs use real time (not deterministic CI time)
      MGC_DETERMINISTIC: ""
      MGC_FIXED_TIME: ""

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Preflight
        shell: bash
        run: |
          set -euxo pipefail
          whoami
          id
          uname -a || true
          df -h || true
          timedatectl || true

          echo "MGC_DB=$MGC_DB"
          ls -la "$(dirname "$MGC_DB")" || true
          test -f "$MGC_DB"

          echo "MGC_FS_PROVIDER_DIR=$MGC_FS_PROVIDER_DIR"
          ls -la "$MGC_FS_PROVIDER_DIR" || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install dependencies
        shell: bash
        run: |
          set -euxo pipefail
          python -m pip install -U pip
          python -m pip install -e .
          python -c "import mgc; print('mgc:', mgc.__file__)"

      - name: DB migrate
        shell: bash
        run: |
          set -euxo pipefail
          python -m mgc.main --db "$MGC_DB" db migrate
          python -m mgc.main --db "$MGC_DB" db status --json

      - name: Ensure stub source track files exist (from prod DB)
        shell: bash
        run: |
          set -euxo pipefail

          # Print required paths (full_path + preview_path)
          python - <<'PY' > /tmp/mgc_required_track_paths.txt
          import sqlite3
          db = "$MGC_DB"
          con = sqlite3.connect(db)
          cur = con.cursor()
          paths = set()
          for col in ("full_path", "preview_path"):
            try:
              rows = cur.execute(f"SELECT {col} FROM tracks").fetchall()
            except Exception:
              rows = []
            for (p,) in rows:
              if p:
                paths.add(str(p))
          con.close()
          for p in sorted(paths):
            print(p)
          PY

          # Create missing WAVs where needed (use sudo when necessary)
          while IFS= read -r p; do
            [ -n "$p" ] || continue
            if [ -f "$p" ]; then
              continue
            fi

            ext="${p##*.}"
            if [ "$ext" != "wav" ]; then
              echo "[seed_tracks] skipping non-wav path: $p"
              continue
            fi

            echo "[seed_tracks] creating missing wav: $p"
            sudo mkdir -p "$(dirname "$p")"

            python - <<'PY'
            import os, wave, struct
            out = "/tmp/mgc_seed_silence.wav"
            sr = 44100
            seconds = 1.0
            nframes = int(sr * seconds)
            with wave.open(out, "wb") as wf:
              wf.setnchannels(1)
              wf.setsampwidth(2)
              wf.setframerate(sr)
              chunk = struct.pack("<h", 0) * 1024
              remaining = nframes
              while remaining > 0:
                take = min(remaining, 1024)
                wf.writeframes(chunk[: take * 2])
                remaining -= take
            print(out)
            PY

            sudo cp -f /tmp/mgc_seed_silence.wav "$p"
            sudo chmod a+r "$p"
          done < /tmp/mgc_required_track_paths.txt

      - name: Provider check (non-blocking)
        shell: bash
        continue-on-error: true
        run: |
          set -euxo pipefail
          python -m mgc.main --db "$MGC_DB" providers check --json || true

      - name: Run autonomous drop (end-to-end)
        shell: bash
        run: |
          set -euxo pipefail

          OUT_DIR="/tmp/mgc_prod_autonomous"
          WEB_OUT_DIR="/tmp/mgc_prod_autonomous_web"

          rm -rf "$OUT_DIR" "$WEB_OUT_DIR"
          mkdir -p "$OUT_DIR" "$WEB_OUT_DIR"

          # Force stub provider for now (until filesystem audio_pool is populated)
          MGC_PROVIDER=stub python -m mgc.main --db "$MGC_DB" run autonomous \
            --context focus \
            --seed 1 \
            --out-dir "$OUT_DIR" \
            --with-web \
            --web-out-dir "$WEB_OUT_DIR" \
            --contract local \
            --json | tee "$OUT_DIR/autonomous.json"

          echo "Autonomous outputs:"
          find "$OUT_DIR" -maxdepth 3 -type f | sed -n '1,200p' || true
          find "$WEB_OUT_DIR" -maxdepth 3 -type f | sed -n '1,200p' || true

      - name: Upload prod artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scheduled-drop-artifacts-prod
          path: |
            /tmp/mgc_prod_autonomous/**
            /tmp/mgc_prod_autonomous_web/**
            data/submissions/**
          if-no-files-found: warn

      - name: Promote release to /var/lib/mgc/releases
        shell: bash
        run: |
          set -euxo pipefail

          OUT_DIR="/tmp/mgc_prod_autonomous"
          WEB_OUT_DIR="/tmp/mgc_prod_autonomous_web"
          AUTON_JSON="$OUT_DIR/autonomous.json"

          test -f "$AUTON_JSON"

          # Parse drop_id from autonomous.json (no heredoc; avoids YAML indentation pitfalls)
          DROP_ID="$(python -c 'import json; p="'"$AUTON_JSON"'"; obj=json.load(open(p,"r",encoding="utf-8")); print(obj["drop"]["ids"]["drop_id"])')"
          echo "DROP_ID=$DROP_ID"

          RELEASE_ROOT="/var/lib/mgc/releases"
          RELEASE_DIR="$RELEASE_ROOT/$DROP_ID"

          sudo mkdir -p "$RELEASE_DIR/web" "$RELEASE_DIR/submission" "$RELEASE_DIR/run"

          # Copy web bundle
          sudo rsync -a --delete "$WEB_OUT_DIR/" "$RELEASE_DIR/web/"

          # Copy submission artifacts (zip + receipts)
          sudo rsync -a "data/submissions/$DROP_ID/" "$RELEASE_DIR/submission/"

          # Copy run evidence for convenience
          sudo rsync -a "$OUT_DIR/" "$RELEASE_DIR/run/"

          # Update latest symlink atomically
          sudo ln -sfn "$RELEASE_DIR" "$RELEASE_ROOT/latest"

          # Fix perms so nginx can read
          sudo chmod -R a+rX "$RELEASE_DIR"

          echo "Promoted release: $RELEASE_DIR"
          echo "Latest -> $(readlink -f "$RELEASE_ROOT/latest")"
